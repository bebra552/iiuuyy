import asyncio
import aiohttp
from aiohttp import ClientSession, TCPConnector
from urllib.parse import urlparse
import csv
import os
import re
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import random
import ssl
import logging
from typing import Optional

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –±–æ–ª–µ–µ —á–∏—Å—Ç–æ–≥–æ –≤—ã–≤–æ–¥–∞ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
BASE = "https://bigspr.ru"
INPUT_FILE = r"C:\\Users\\User\\Desktop\\ssilki.txt"
OUTPUT_FILE = "companies_full_fast.csv"

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ ---
HTTP_CONCURRENCY = 50
CATEGORY_CONCURRENCY = 8
COMPANY_CONCURRENCY = 10
REQUEST_TIMEOUT = 30
MAX_RETRIES = 3
RETRY_DELAY = 2

# DNS cache TTL for connector (seconds)
DNS_CACHE_TTL = 300

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]


def get_random_headers() -> dict:
    ua = random.choice(USER_AGENTS)
    return {
        "User-Agent": ua,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept-Encoding": "gzip, deflate, br, zstd",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Cache-Control": "max-age=0",
        "DNT": "1",
    }


http_sem = asyncio.Semaphore(HTTP_CONCURRENCY)

# Reuse SSL context and client timeout across requests
SSL_CONTEXT = ssl.create_default_context()
CLIENT_TIMEOUT = aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)

# Precompiled regex for company URL detection
COMPANY_ID_RE = re.compile(r"-\d{6,}")

# Base headers (including a single randomized User-Agent) to be set per session
BASE_HEADERS: Optional[dict] = None

# Try to use uvloop if available (faster and lower CPU overhead)
try:
    import uvloop  # type: ignore
    asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
except Exception:
    pass


def ensure_trailing_slash(url: str) -> str:
    return url if url.endswith("/") else url + "/"


def is_company_url(path: str) -> bool:
    parts = [p for p in path.split("/") if p]
    if len(parts) < 2:
        return False
    return any(COMPANY_ID_RE.search(part) for part in parts)


def extract_city_from_rubric(rubric_url: str) -> str | None:
    try:
        path = urlparse(rubric_url).path
        parts = [p for p in path.split("/") if p]
        return parts[0] if parts else None
    except Exception:
        return None


async def fetch_text(session: ClientSession, url: str) -> str | None:
    for attempt in range(MAX_RETRIES):
        try:
            parsed = urlparse(url)
            # Use session-level headers and only add Referer per request
            headers = (BASE_HEADERS or get_random_headers()).copy()
            headers["Referer"] = f"{parsed.scheme}://{parsed.netloc}/"

            async with http_sem:
                async with session.get(url, headers=headers) as resp:
                    # Skip expensive charset detection; site is UTF-8
                    text = await resp.text(encoding='utf-8', errors='ignore')
                    if "document.cookie='beget=begetok'" in text:
                        logging.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞—â–∏—Ç–∞ Beget. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é cookie –¥–ª—è {url}")
                        session.cookie_jar.update_cookies({'beget': 'begetok'})
                        await asyncio.sleep(0.5)
                        continue
                    if resp.status == 200:
                        return text
                    logging.warning(f"–°—Ç–∞—Ç—É—Å {resp.status} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
        if attempt < MAX_RETRIES - 1:
            await asyncio.sleep(RETRY_DELAY)
    logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å {url} –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫.")
    return None


async def parse_sitemap_xml(session: ClientSession, url: str) -> list[str]:
    logging.info(f"–ó–∞–≥—Ä—É–∂–∞—é sitemap: {url}")
    text = await fetch_text(session, url)
    if not text: return []
    try:
        root = ET.fromstring(text.encode('utf-8'))
        locs = [el.text.strip() for el in root.findall(".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc") if
                el.text]
        logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(locs)} —Å—Å—ã–ª–æ–∫ –≤ sitemap: {url}")
        return locs
    except ET.ParseError:
        logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML –≤ {url}.")
        return []


async def discover_city_rubrics_from_input(session: ClientSession, input_url: str) -> list[str]:
    url = input_url.strip()
    if url.endswith(".xml") and "/sitemap/" in url:
        return await parse_sitemap_xml(session, url)
    logging.warning(f"–ù–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–∏–ø —Å—Å—ã–ª–∫–∏: {url}")
    return []


async def parse_company(session: ClientSession, company_url: str, city: str | None, rubric_url: str) -> dict | None:
    html_text = await fetch_text(session, company_url)
    if not html_text:
        return None

    try:
        soup = BeautifulSoup(html_text, "lxml")
        data = {
            "city": city,
            "rubric_url": rubric_url,
            "url": company_url,
            "name": None,
            "address": None,
            "phones": None,
            "schedule": None,
            "website": None,
            "socials": None,
            "email": None,
            "categories": None
        }

        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
        h1 = soup.select_one("h1")
        if h1:
            data["name"] = h1.get_text(strip=True)
        else:
            data["name"] = "–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

        # –ò—â–µ–º –∫–æ–Ω—Ç–∞–∫—Ç—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–π –ø–æ–∏—Å–∫
        contacts_panel = soup.select_one(".panel-body .item-text")
        if not contacts_panel:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
            contacts_panel = soup.select_one(".item-text")
            if not contacts_panel:
                contacts_panel = soup.select_one(".panel-body")

        # –°–æ–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –ª–∏–±–æ –∏–∑ –ø–∞–Ω–µ–ª–∏ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤, –ª–∏–±–æ –∏–∑ –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (fallback)
        candidate_paragraphs = contacts_panel.find_all("p") if contacts_panel else soup.find_all("p")
        if candidate_paragraphs:
            for p_tag in candidate_paragraphs:
                text = p_tag.get_text(" ", strip=True)
                # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º –∏–∫–æ–Ω–æ–∫ –±–µ–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ HTML-—Å—Ç—Ä–æ–∫—É
                icon_span = p_tag.find("span", class_=True)
                icon_classes = icon_span.get("class", []) if icon_span else []

                matched = False
                if "glyphicon-map-marker" in icon_classes:
                    data["address"] = text; matched = True
                elif "glyphicon-earphone" in icon_classes:
                    data["phones"] = text; matched = True
                elif "glyphicon-time" in icon_classes:
                    data["schedule"] = text; matched = True
                elif "glyphicon-share" in icon_classes:
                    data["website"] = text; matched = True
                elif "glyphicon-link" in icon_classes:
                    links = [a.get("href") for a in p_tag.find_all("a", href=True)]
                    data["socials"] = ", ".join(links) if links else text; matched = True
                elif "glyphicon-envelope" in icon_classes:
                    data["email"] = text; matched = True
                elif "glyphicon-list" in icon_classes:
                    cats = [a.get_text(strip=True) for a in p_tag.find_all("a")]
                    data["categories"] = ", ".join(cats) if cats else text; matched = True

                # –†–µ–¥–∫–∏–π —Å–ª—É—á–∞–π: –∑–∞–ø–∞—Å–Ω–æ–π –ø—É—Ç—å —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –ø–æ HTML (–º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
                if not matched:
                    p_html = str(p_tag)
                    if "glyphicon-map-marker" in p_html:
                        data["address"] = text
                    elif "glyphicon-earphone" in p_html:
                        data["phones"] = text
                    elif "glyphicon-time" in p_html:
                        data["schedule"] = text
                    elif "glyphicon-share" in p_html:
                        data["website"] = text
                    elif "glyphicon-link" in p_html:
                        links = [a.get("href") for a in p_tag.find_all("a", href=True)]
                        data["socials"] = ", ".join(links) if links else text
                    elif "glyphicon-envelope" in p_html:
                        data["email"] = text
                    elif "glyphicon-list" in p_html:
                        cats = [a.get_text(strip=True) for a in p_tag.find_all("a")]
                        data["categories"] = ", ".join(cats) if cats else text

        logging.debug(
            "    ‚Ü≥ –°–æ–±—Ä–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–∏: %s | –ê–¥—Ä–µ—Å: %s | –¢–µ–ª–µ—Ñ–æ–Ω: %s",
            data['name'], bool(data['address']), bool(data['phones'])
        )
        return data
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –∫–æ–º–ø–∞–Ω–∏–∏ {company_url}: {e}")
        return None


async def parse_category(session: ClientSession, rubric_url: str, csv_writer, lock: asyncio.Lock):
    rubric_url = ensure_trailing_slash(rubric_url)
    city = extract_city_from_rubric(rubric_url)
    rubric_name = rubric_url.rstrip('/').split('/')[-1]

    logging.info(f"–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ä—É–±—Ä–∏–∫–∏: {city}/{rubric_name}")
    page = 1
    total_companies_in_rubric = 0

    while True:
        page_url = f"{rubric_url}page-{page}/" if page > 1 else rubric_url
        html_text = await fetch_text(session, page_url)
        if not html_text:
            break

        soup = BeautifulSoup(html_text, "lxml")
        company_links = [a['href'] for a in soup.select(".panel.panel-info.object .panel-heading a[href]")]

        if not company_links:
            if page > 1:
                logging.info(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü —Ä—É–±—Ä–∏–∫–∏ {city}/{rubric_name} –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}.")
            break

        # –°–æ–±–∏—Ä–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏ —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        seen_urls = set()
        company_urls = []
        for href in company_links:
            abs_url = BASE + href if href.startswith('/') else href
            if abs_url not in seen_urls:
                seen_urls.add(abs_url)
                company_urls.append(abs_url)
        logging.info(
            f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} [{city}/{rubric_name}]: –Ω–∞–π–¥–µ–Ω–æ {len(company_urls)} –∫–æ–º–ø–∞–Ω–∏–π. –ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö...")

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–º–ø–∞–Ω–∏–π
        sem = asyncio.Semaphore(COMPANY_CONCURRENCY)

        async def _parse_with_sem(url):
            async with sem:
                return await parse_company(session, url, city, rubric_url)

        tasks = [_parse_with_sem(url) for url in company_urls]
        results = await asyncio.gather(*tasks)
        valid_results = [res for res in results if res]

        if valid_results:
            async with lock:
                csv_writer.writerows(valid_results)
            logging.info(
                f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} [{city}/{rubric_name}]: –°–û–•–†–ê–ù–ï–ù–û {len(valid_results)}/{len(company_urls)} –∫–æ–º–ø–∞–Ω–∏–π –≤ CSV.")
            total_companies_in_rubric += len(valid_results)

        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏, –Ω–æ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º —Ä–∞–Ω—å—à–µ
        await asyncio.sleep(random.uniform(0.02, 0.05))
        page += 1

    logging.info(f"‚úÖ –†—É–±—Ä–∏–∫–∞ {city}/{rubric_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {total_companies_in_rubric} –∫–æ–º–ø–∞–Ω–∏–π.")


async def main():
    start_time = asyncio.get_event_loop().time()
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ BigSpr...")

    if not os.path.exists(INPUT_FILE):
        logging.critical(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {INPUT_FILE}")
        return

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        input_lines = [line.strip() for line in f if line.strip()]

    connector = TCPConnector(
        limit=HTTP_CONCURRENCY,
        limit_per_host=HTTP_CONCURRENCY,
        ttl_dns_cache=DNS_CACHE_TTL,
        ssl=SSL_CONTEXT,
        enable_cleanup_closed=True,
    )

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –µ–¥–∏–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –≤—Å–µ–π —Å–µ—Å—Å–∏–∏ (—Å–ª—É—á–∞–π–Ω—ã–π User-Agent –æ–¥–∏–Ω —Ä–∞–∑)
    global BASE_HEADERS
    BASE_HEADERS = get_random_headers()

    async with ClientSession(connector=connector, timeout=CLIENT_TIMEOUT, headers=BASE_HEADERS) as session:
        logging.info("--- –≠–¢–ê–ü 1: –°–±–æ—Ä URL —Ä—É–±—Ä–∏–∫ –∏–∑ sitemap-–æ–≤ ---")
        discovery_tasks = [discover_city_rubrics_from_input(session, line) for line in input_lines]
        results = await asyncio.gather(*discovery_tasks)

        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Ä—É–±—Ä–∏–∫ –±–µ–∑ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è)
        rubric_urls = []
        seen_rubrics = set()
        for url_list in results:
            for url in url_list:
                if is_company_url(urlparse(url).path):
                    continue
                if url not in seen_rubrics:
                    seen_rubrics.add(url)
                    rubric_urls.append(url)

        if not rubric_urls:
            logging.critical("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π —Ä—É–±—Ä–∏–∫–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞.")
            return

        logging.info(f"‚úÖ –ò–¢–û–ì –≠–¢–ê–ü–ê 1: –ù–∞–π–¥–µ–Ω–æ {len(rubric_urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä—É–±—Ä–∏–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞.")

        fieldnames = ["city", "rubric_url", "url", "name", "address", "phones", "schedule", "website", "socials",
                      "email", "categories"]
        csv_lock = asyncio.Lock()

        logging.info(f"\n--- –≠–¢–ê–ü 2: –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ —Ä—É–±—Ä–∏–∫ ---")
        try:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—é –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ –¥–∏—Å–∫
            with open(OUTPUT_FILE, "w", newline="", encoding="utf-8", buffering=1024*1024) as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                logging.info(f"–°–æ–∑–¥–∞–Ω CSV —Ñ–∞–π–ª: {OUTPUT_FILE}")

                sem_category = asyncio.Semaphore(CATEGORY_CONCURRENCY)
                progress_counter = 0
                progress_lock = asyncio.Lock()

                async def run_category(rurl):
                    nonlocal progress_counter
                    async with sem_category:
                        await parse_category(session, rurl, writer, csv_lock)
                    async with progress_lock:
                        progress_counter += 1
                        progress = (progress_counter / len(rubric_urls)) * 100
                        logging.info(
                            f"üìà –ü–†–û–ì–†–ï–°–°: {progress_counter}/{len(rubric_urls)} ({progress:.1f}%) | –í—Ä–µ–º—è: {asyncio.get_event_loop().time() - start_time:.0f}—Å")

                await asyncio.gather(*[run_category(r) for r in rubric_urls])
        except IOError as e:
            logging.critical(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª {OUTPUT_FILE}: {e}")

    total_time = asyncio.get_event_loop().time() - start_time
    try:
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            total_companies = sum(1 for _ in f) - 1
    except FileNotFoundError:
        total_companies = 0

    logging.info("\n" + "=" * 50 + "\nüéâ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!\n" + "=" * 50)
    logging.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä—É–±—Ä–∏–∫: {len(rubric_urls)}")
    logging.info(f"üè¢ –°–æ–±—Ä–∞–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {total_companies}")
    logging.info(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f} —Å–µ–∫—É–Ω–¥")
    if total_time > 0:
        logging.info(f"‚ö° –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {total_companies / total_time:.1f} –∫–æ–º–ø–∞–Ω–∏–π/—Å–µ–∫")
    logging.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {OUTPUT_FILE}")


if __name__ == "__main__":
    asyncio.run(main())
