import asyncio
import aiohttp
from aiohttp import ClientSession, TCPConnector
from urllib.parse import urlparse, parse_qs, unquote
import csv
import os
import re
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import random
import ssl
import logging
from typing import Optional

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –±–æ–ª–µ–µ —á–∏—Å—Ç–æ–≥–æ –≤—ã–≤–æ–¥–∞ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
BASE = "https://bigspr.ru"
INPUT_FILE = r"C:\\Users\\User\\Desktop\\ssilki.txt"
OUTPUT_FILE = "companies_full_fast.csv"

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ ---
HTTP_CONCURRENCY = 50
CATEGORY_CONCURRENCY = 8
COMPANY_CONCURRENCY = 10
REQUEST_TIMEOUT = 30
MAX_RETRIES = 3
RETRY_DELAY = 2

# DNS cache TTL for connector (seconds)
DNS_CACHE_TTL = 300

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]


def get_random_headers() -> dict:
    ua = random.choice(USER_AGENTS)
    return {
        "User-Agent": ua,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept-Encoding": "gzip, deflate, br, zstd",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Cache-Control": "max-age=0",
        "DNT": "1",
    }


http_sem = asyncio.Semaphore(HTTP_CONCURRENCY)

# Reuse SSL context and client timeout across requests
SSL_CONTEXT = ssl.create_default_context()
CLIENT_TIMEOUT = aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)

# Precompiled regex for company URL detection
COMPANY_ID_RE = re.compile(r"-\d{6,}")
URL_IN_TEXT_RE = re.compile(r"https?://[^\s\"\)\]]+", re.IGNORECASE)

# Base headers (including a single randomized User-Agent) to be set per session
BASE_HEADERS: Optional[dict] = None

# Try to use uvloop if available (faster and lower CPU overhead)
try:
    import uvloop  # type: ignore
    asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
except Exception:
    pass


def ensure_trailing_slash(url: str) -> str:
    return url if url.endswith("/") else url + "/"


def is_company_url(path: str) -> bool:
    parts = [p for p in path.split("/") if p]
    if len(parts) < 2:
        return False
    return any(COMPANY_ID_RE.search(part) for part in parts)


def extract_city_from_rubric(rubric_url: str) -> str | None:
    try:
        path = urlparse(rubric_url).path
        parts = [p for p in path.split("/") if p]
        return parts[0] if parts else None
    except Exception:
        return None


def normalize_domain(domain: str | None) -> str | None:
    if not domain:
        return None
    domain = domain.strip().lower()
    if domain.startswith("www."):
        domain = domain[4:]
    return domain or None


def extract_domain_from_text(text: str | None) -> str | None:
    if not text:
        return None
    # Handle "green_url" style: domain‚Ä∫title
    if "‚Ä∫" in text:
        candidate = text.split("‚Ä∫", 1)[0].strip()
        return normalize_domain(candidate)
    # Find first http(s) URL
    match = URL_IN_TEXT_RE.search(text)
    if match:
        return normalize_domain(urlparse(match.group(0)).netloc)
    # Fallback: bare domain separated by spaces
    parts = text.split()
    for part in parts:
        if "." in part and not part.startswith("http"):
            # Try parse as domain
            maybe = part.strip("()[]{};,'\"")
            if "." in maybe:
                return normalize_domain(maybe)
    return None


def pick_target_domain_from_href(href: str | None) -> str | None:
    if not href:
        return None
    parsed = urlparse(href)
    host = parsed.netloc.lower()
    # Try to resolve known redirect patterns
    if host.endswith("yabs.yandex.ru") or host.endswith("yandex.ru"):
        qs = parse_qs(parsed.query)
        for key in ("url", "u", "target", "to"):
            if key in qs and qs[key]:
                try:
                    target = unquote(qs[key][0])
                    return normalize_domain(urlparse(target).netloc or target)
                except Exception:
                    continue
    return normalize_domain(host)


async def fetch_text(session: ClientSession, url: str) -> str | None:
    for attempt in range(MAX_RETRIES):
        try:
            parsed = urlparse(url)
            # Use session-level headers and only add Referer per request
            headers = (BASE_HEADERS or get_random_headers()).copy()
            headers["Referer"] = f"{parsed.scheme}://{parsed.netloc}/"

            async with http_sem:
                async with session.get(url, headers=headers) as resp:
                    # Skip expensive charset detection; site is UTF-8
                    text = await resp.text(encoding='utf-8', errors='ignore')
                    if "document.cookie='beget=begetok'" in text:
                        logging.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞—â–∏—Ç–∞ Beget. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é cookie –¥–ª—è {url}")
                        session.cookie_jar.update_cookies({'beget': 'begetok'})
                        await asyncio.sleep(0.5)
                        continue
                    if resp.status == 200:
                        return text
                    logging.warning(f"–°—Ç–∞—Ç—É—Å {resp.status} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
        if attempt < MAX_RETRIES - 1:
            await asyncio.sleep(RETRY_DELAY)
    logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å {url} –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫.")
    return None


async def parse_sitemap_xml(session: ClientSession, url: str) -> list[str]:
    logging.info(f"–ó–∞–≥—Ä—É–∂–∞—é sitemap: {url}")
    text = await fetch_text(session, url)
    if not text: return []
    try:
        root = ET.fromstring(text.encode('utf-8'))
        locs = [el.text.strip() for el in root.findall(".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc") if
                el.text]
        logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(locs)} —Å—Å—ã–ª–æ–∫ –≤ sitemap: {url}")
        return locs
    except ET.ParseError:
        logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML –≤ {url}.")
        return []


async def discover_city_rubrics_from_input(session: ClientSession, input_url: str) -> list[str]:
    url = input_url.strip()
    if url.endswith(".xml") and "/sitemap/" in url:
        return await parse_sitemap_xml(session, url)
    logging.warning(f"–ù–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–∏–ø —Å—Å—ã–ª–∫–∏: {url}")
    return []


async def parse_company(session: ClientSession, company_url: str, city: str | None, rubric_url: str) -> dict | None:
    html_text = await fetch_text(session, company_url)
    if not html_text:
        return None

    try:
        soup = BeautifulSoup(html_text, "lxml")
        data = {
            "city": city,
            "rubric_url": rubric_url,
            "url": company_url,
            "name": None,
            "address": None,
            "phones": None,
            "schedule": None,
            "website": None,
            "domain": None,
            "socials": None,
            "email": None,
            "categories": None
        }

        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
        h1 = soup.select_one("h1")
        if h1:
            data["name"] = h1.get_text(strip=True)
        else:
            data["name"] = "–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

        # –ò—â–µ–º –∫–æ–Ω—Ç–∞–∫—Ç—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–π –ø–æ–∏—Å–∫
        contacts_panel = soup.select_one(".panel-body .item-text")
        if not contacts_panel:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
            contacts_panel = soup.select_one(".item-text")
            if not contacts_panel:
                contacts_panel = soup.select_one(".panel-body")

        # –°–æ–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –ª–∏–±–æ –∏–∑ –ø–∞–Ω–µ–ª–∏ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤, –ª–∏–±–æ –∏–∑ –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (fallback)
        candidate_paragraphs = contacts_panel.find_all("p") if contacts_panel else soup.find_all("p")
        if candidate_paragraphs:
            for p_tag in candidate_paragraphs:
                text = p_tag.get_text(" ", strip=True)
                # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º –∏–∫–æ–Ω–æ–∫ –±–µ–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ HTML-—Å—Ç—Ä–æ–∫—É
                icon_span = p_tag.find("span", class_=True)
                icon_classes = icon_span.get("class", []) if icon_span else []

                matched = False
                if "glyphicon-map-marker" in icon_classes:
                    data["address"] = text; matched = True
                elif "glyphicon-earphone" in icon_classes:
                    data["phones"] = text; matched = True
                elif "glyphicon-time" in icon_classes:
                    data["schedule"] = text; matched = True
                elif "glyphicon-share" in icon_classes:
                    link = p_tag.find("a", href=True)
                    href = link.get("href") if link else None
                    data["website"] = href or text; matched = True
                    data["domain"] = pick_target_domain_from_href(href) or extract_domain_from_text(text)
                elif "glyphicon-link" in icon_classes:
                    links = [a.get("href") for a in p_tag.find_all("a", href=True)]
                    data["socials"] = ", ".join(links) if links else text; matched = True
                elif "glyphicon-envelope" in icon_classes:
                    data["email"] = text; matched = True
                elif "glyphicon-list" in icon_classes:
                    cats = [a.get_text(strip=True) for a in p_tag.find_all("a")]
                    data["categories"] = ", ".join(cats) if cats else text; matched = True

                # –†–µ–¥–∫–∏–π —Å–ª—É—á–∞–π: –∑–∞–ø–∞—Å–Ω–æ–π –ø—É—Ç—å —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –ø–æ HTML (–º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
                if not matched:
                    p_html = str(p_tag)
                    if "glyphicon-map-marker" in p_html:
                        data["address"] = text
                    elif "glyphicon-earphone" in p_html:
                        data["phones"] = text
                    elif "glyphicon-time" in p_html:
                        data["schedule"] = text
                    elif "glyphicon-share" in p_html:
                        data["website"] = text
                        if not data.get("domain"):
                            data["domain"] = extract_domain_from_text(text)
                    elif "glyphicon-link" in p_html:
                        links = [a.get("href") for a in p_tag.find_all("a", href=True)]
                        data["socials"] = ", ".join(links) if links else text
                    elif "glyphicon-envelope" in p_html:
                        data["email"] = text
                    elif "glyphicon-list" in p_html:
                        cats = [a.get_text(strip=True) for a in p_tag.find_all("a")]
                        data["categories"] = ", ".join(cats) if cats else text

        logging.debug(
            "    ‚Ü≥ –°–æ–±—Ä–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–∏: %s | –ê–¥—Ä–µ—Å: %s | –¢–µ–ª–µ—Ñ–æ–Ω: %s",
            data['name'], bool(data['address']), bool(data['phones'])
        )
        return data
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –∫–æ–º–ø–∞–Ω–∏–∏ {company_url}: {e}")
        return None


async def parse_category(session: ClientSession, rubric_url: str, csv_writer, lock: asyncio.Lock):
    rubric_url = ensure_trailing_slash(rubric_url)
    city = extract_city_from_rubric(rubric_url)
    rubric_name = rubric_url.rstrip('/').split('/')[-1]

    logging.info(f"–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ä—É–±—Ä–∏–∫–∏: {city}/{rubric_name}")
    page = 1
    total_companies_in_rubric = 0

    while True:
        page_url = f"{rubric_url}page-{page}/" if page > 1 else rubric_url
        html_text = await fetch_text(session, page_url)
        if not html_text:
            break

        soup = BeautifulSoup(html_text, "lxml")
        company_links = [a['href'] for a in soup.select(".panel.panel-info.object .panel-heading a[href]")]

        if not company_links:
            if page > 1:
                logging.info(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü —Ä—É–±—Ä–∏–∫–∏ {city}/{rubric_name} –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}.")
            break

        # –°–æ–±–∏—Ä–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏ —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        seen_urls = set()
        company_urls = []
        for href in company_links:
            abs_url = BASE + href if href.startswith('/') else href
            if abs_url not in seen_urls:
                seen_urls.add(abs_url)
                company_urls.append(abs_url)
        logging.info(
            f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} [{city}/{rubric_name}]: –Ω–∞–π–¥–µ–Ω–æ {len(company_urls)} –∫–æ–º–ø–∞–Ω–∏–π. –ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö...")

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–º–ø–∞–Ω–∏–π
        sem = asyncio.Semaphore(COMPANY_CONCURRENCY)

        async def _parse_with_sem(url):
            async with sem:
                return await parse_company(session, url, city, rubric_url)

        tasks = [_parse_with_sem(url) for url in company_urls]
        results = await asyncio.gather(*tasks)
        valid_results = [res for res in results if res]

        if valid_results:
            async with lock:
                csv_writer.writerows(valid_results)
            logging.info(
                f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} [{city}/{rubric_name}]: –°–û–•–†–ê–ù–ï–ù–û {len(valid_results)}/{len(company_urls)} –∫–æ–º–ø–∞–Ω–∏–π –≤ CSV.")
            total_companies_in_rubric += len(valid_results)

        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏, –Ω–æ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º —Ä–∞–Ω—å—à–µ
        await asyncio.sleep(random.uniform(0.02, 0.05))
        page += 1

    logging.info(f"‚úÖ –†—É–±—Ä–∏–∫–∞ {city}/{rubric_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {total_companies_in_rubric} –∫–æ–º–ø–∞–Ω–∏–π.")


async def process_single_input(session: ClientSession, line: str) -> list[str]:
    line = line.strip()
    if not line:
        return []
    # If it's a BigSpr sitemap
    if line.endswith(".xml") and "/sitemap/" in line:
        return await parse_sitemap_xml(session, line)
    # If it's a BigSpr rubric or company URL
    parsed = urlparse(line)
    if parsed.netloc.endswith("bigspr.ru"):
        return [line]
    # Otherwise: treat as arbitrary search/result URL; we do not follow it here
    logging.warning(f"–ü—Ä–æ–ø—É—Å–∫–∞—é –Ω–µ—Ä–æ–¥–Ω–æ–π URL: {line}")
    return []


async def main():
    start_time = asyncio.get_event_loop().time()
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ BigSpr...")

    if not os.path.exists(INPUT_FILE):
        logging.critical(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {INPUT_FILE}")
        return

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        input_lines = [line.strip() for line in f if line.strip()]

    connector = TCPConnector(
        limit=HTTP_CONCURRENCY,
        limit_per_host=HTTP_CONCURRENCY,
        ttl_dns_cache=DNS_CACHE_TTL,
        ssl=SSL_CONTEXT,
        enable_cleanup_closed=True,
    )

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –µ–¥–∏–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –≤—Å–µ–π —Å–µ—Å—Å–∏–∏ (—Å–ª—É—á–∞–π–Ω—ã–π User-Agent –æ–¥–∏–Ω —Ä–∞–∑)
    global BASE_HEADERS
    BASE_HEADERS = get_random_headers()

    async with ClientSession(connector=connector, timeout=CLIENT_TIMEOUT, headers=BASE_HEADERS) as session:
        logging.info("--- –≠–¢–ê–ü 1: –°–±–æ—Ä –≤—Ö–æ–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ (sitemap/—Ä—É–±—Ä–∏–∫–∏/–∫–æ–º–ø–∞–Ω–∏–∏) ---")
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø—Ä—è–º—ã—Ö —Å—Å—ã–ª–æ–∫: sitemap, —Ä—É–±—Ä–∏–∫–∞, –∫–∞—Ä—Ç–æ—á–∫–∞ –∫–æ–º–ø–∞–Ω–∏–∏
        tasks = [process_single_input(session, line) for line in input_lines]
        raw_urls_nested = await asyncio.gather(*tasks)

        # –ü–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞
        seen = set()
        all_urls: list[str] = []
        for lst in raw_urls_nested:
            for u in lst:
                if u not in seen:
                    seen.add(u)
                    all_urls.append(u)

        # –î–µ–ª–∏–º –Ω–∞ —Ä—É–±—Ä–∏–∫–∏ –∏ –∫–∞—Ä—Ç–æ—á–∫–∏
        rubric_urls: list[str] = []
        company_urls_direct: list[str] = []
        for u in all_urls:
            if is_company_url(urlparse(u).path):
                company_urls_direct.append(u)
            else:
                rubric_urls.append(u)

        total_inputs = len(rubric_urls) + len(company_urls_direct)
        if total_inputs == 0:
            logging.critical("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞.")
            return

        logging.info(f"‚úÖ –ò–¢–û–ì –≠–¢–ê–ü–ê 1: –†—É–±—Ä–∏–∫: {len(rubric_urls)} | –ö–æ–º–ø–∞–Ω–∏–π –Ω–∞–ø—Ä—è–º—É—é: {len(company_urls_direct)}")

        fieldnames = ["city", "rubric_url", "url", "name", "address", "phones", "schedule", "website", "domain", "socials",
                      "email", "categories"]
        csv_lock = asyncio.Lock()

        logging.info(f"\n--- –≠–¢–ê–ü 2: –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–ø–∞–Ω–∏–π ---")
        try:
            with open(OUTPUT_FILE, "w", newline="", encoding="utf-8", buffering=1024*1024) as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                logging.info(f"–°–æ–∑–¥–∞–Ω CSV —Ñ–∞–π–ª: {OUTPUT_FILE}")

                sem_category = asyncio.Semaphore(CATEGORY_CONCURRENCY)
                progress_counter = 0
                progress_lock = asyncio.Lock()

                async def run_category(rurl):
                    nonlocal progress_counter
                    async with sem_category:
                        await parse_category(session, rurl, writer, csv_lock)
                    async with progress_lock:
                        progress_counter += 1
                        total = len(rubric_urls) + len(company_urls_direct)
                        progress = (progress_counter / total) * 100
                        logging.info(
                            f"üìà –ü–†–û–ì–†–ï–°–°: {progress_counter}/{total} ({progress:.1f}%) | –í—Ä–µ–º—è: {asyncio.get_event_loop().time() - start_time:.0f}—Å")

                # 1) –°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∞–µ–º —Ä—É–±—Ä–∏–∫–∏ (—Å—Ç—Ä–∞–Ω–∏—á–Ω—ã–π —Å–±–æ—Ä)
                await asyncio.gather(*[run_category(r) for r in rubric_urls])

                # 2) –ó–∞—Ç–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–º–ø–∞–Ω–∏–∏
                if company_urls_direct:
                    logging.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –ø—Ä—è–º—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –∫–æ–º–ø–∞–Ω–∏–π: {len(company_urls_direct)}")
                    sem_company = asyncio.Semaphore(COMPANY_CONCURRENCY)

                    async def run_company(curl: str):
                        nonlocal progress_counter
                        async with sem_company:
                            data = await parse_company(session, curl, extract_city_from_rubric(curl), curl)
                            if data:
                                async with csv_lock:
                                    writer.writerow(data)
                        async with progress_lock:
                            progress_counter += 1
                            total = len(rubric_urls) + len(company_urls_direct)
                            progress = (progress_counter / total) * 100
                            logging.info(
                                f"üìà –ü–†–û–ì–†–ï–°–°: {progress_counter}/{total} ({progress:.1f}%) | –í—Ä–µ–º—è: {asyncio.get_event_loop().time() - start_time:.0f}—Å")

                    await asyncio.gather(*[run_company(u) for u in company_urls_direct])
        except IOError as e:
            logging.critical(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª {OUTPUT_FILE}: {e}")

    total_time = asyncio.get_event_loop().time() - start_time
    try:
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            total_companies = sum(1 for _ in f) - 1
    except FileNotFoundError:
        total_companies = 0

    logging.info("\n" + "=" * 50 + "\nüéâ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!\n" + "=" * 50)
    logging.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä—É–±—Ä–∏–∫: {len(rubric_urls)}")
    logging.info(f"üè¢ –°–æ–±—Ä–∞–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {total_companies}")
    logging.info(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f} —Å–µ–∫—É–Ω–¥")
    if total_time > 0:
        logging.info(f"‚ö° –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {total_companies / total_time:.1f} –∫–æ–º–ø–∞–Ω–∏–π/—Å–µ–∫")
    logging.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {OUTPUT_FILE}")


if __name__ == "__main__":
    asyncio.run(main())