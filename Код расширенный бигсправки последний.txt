import pandas as pd
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import time
from urllib.parse import quote_plus, urlparse
import json
from datetime import datetime
import sys
import traceback
import re
import requests
from typing import Dict, Optional, List

# =================== НАСТРОЙКИ ===================
SEARCH_QUERY = "Сэндвич панели"
PAGES_TO_SCRAPE = 1  # сколько страниц собирать (минимум 1)
CSV_FILE_NAME = "yandex_search_results.csv"
CSV_WITH_WHOIS_NAME = "yandex_search_results_with_whois.csv"
SLOW_MO_MS = 80  # замедление действий браузера
WAIT_SELECTOR_TIMEOUT = 15000  # мс ожидания появления результатов
PAUSE_AFTER_NAV_SEC = 2.0  # пауза после перехода на страницу
AUTOSAVE_AFTER_EACH_PAGE = False  # True чтобы сохранять CSV после каждой страницы
WHOIS_REQUEST_DELAY = 1.0  # задержка между запросами к whois.ru (секунды)
# =================================================

# Селекторы
RESULT_ITEM_SELECTOR = 'li.serp-item'
ALT_RESULT_SELECTORS = [
    'div.serp-item',
    'div.organic',
    'article.serp-item',
    'li.organic'
]
NEXT_BTN_SELECTORS = [
    'a.Pager-Item_type_next',
    'a.Link[aria-label="Следующая страница"]',
    'a[aria-label="Следующая страница"]',
    'a.pager__item_kind_next',
    'a.pager__item_kind_next_link'
]
PHONE_BUTTON_SELECTOR = 'button.CoveredPhone-Button'
PHONE_SPAN_SELECTOR = 'span.CoveredPhone'
PHONE_FULL_SELECTOR = 'span.CoveredPhone-Full'


def extract_domain_from_url(url: str) -> Optional[str]:
    """Извлекает чистый домен из URL или green_url"""
    if not url:
        return None

    try:
        # Если это yandex redirect URL, пропускаем
        if 'yabs.yandex.ru' in url or 'yandex.ru' in url:
            return None

        # Убираем все после › символа (для green_url)
        if '›' in url:
            url = url.split('›')[0]

        # Добавляем протокол если его нет
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        parsed = urlparse(url)
        domain = parsed.netloc

        # Убираем www. и другие субдомены, кроме основного
        if domain.startswith('www.'):
            domain = domain[4:]

        # Проверяем что это валидный домен
        if '.' not in domain or len(domain) < 4:
            return None

        return domain.lower()
    except Exception:
        return None


def get_whois_data(domain: str) -> Dict[str, Optional[str]]:
    """Получает данные WHOIS для домена через whois.ru с использованием браузера"""

    whois_data = {
        'domain': domain,
        'citation_index': None,
        'alexa_rating': None,
        'registrar': None,
        'registration_date': None,
        'expiration_date': None,
        'days_until_expiration': None,
        'check_date': None,
        'external_links': None,
        'internal_links': None,
        'total_anchors': None,
        'outgoing_anchors': None,
        'domain_links': None,
        'page_title': None,
        'page_description': None,
        'whois_error': None
    }

    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()

            # Заходим на страницу домена
            whois_url = f'https://whois.ru/{domain}'
            page.goto(whois_url, wait_until='domcontentloaded')

            # Ждем загрузки данных (проверяем появление блоков с информацией)
            try:
                page.wait_for_selector('.list-group-item', timeout=10000)
            except:
                # Если данные не загрузились, возможно нужно нажать кнопку
                try:
                    button = page.query_selector('#whois_btn')
                    if button:
                        button.click()
                        page.wait_for_selector('.list-group-item', timeout=10000)
                except:
                    pass

            # Получаем HTML и парсим
            html_content = page.content()
            browser.close()

        soup = BeautifulSoup(html_content, 'html.parser')

        # Ищем блоки с данными
        list_groups = soup.find_all('ul', class_='list-group')

        for ul in list_groups:
            items = ul.find_all('li', class_='list-group-item')

            for item in items:
                text = item.get_text().strip()
                strong = item.find('strong')

                if not strong:
                    continue

                value = strong.get_text().strip()

                if 'Индекс цитирования' in text:
                    whois_data['citation_index'] = value
                elif 'Рейтинг Alexa' in text:
                    whois_data['alexa_rating'] = value
                elif 'Регистратор домена' in text:
                    whois_data['registrar'] = value
                elif 'Дата регистрации' in text:
                    whois_data['registration_date'] = value
                elif 'Дата окончания' in text:
                    whois_data['expiration_date'] = value
                elif 'Закончится через' in text:
                    whois_data['days_until_expiration'] = value
                elif 'Дата проверки' in text:
                    whois_data['check_date'] = value
                elif 'Внешние ссылки домена' in text:
                    whois_data['external_links'] = value
                elif 'Внутренние ссылки' in text:
                    whois_data['internal_links'] = value
                elif 'Кол-во найденных анкоров' in text:
                    whois_data['total_anchors'] = value
                elif 'Кол-во исходящих анкоров' in text:
                    whois_data['outgoing_anchors'] = value
                elif 'Кол-во ссылок на домене' in text:
                    whois_data['domain_links'] = value
                elif 'Title страницы' in text:
                    whois_data['page_title'] = value
                elif 'Description страницы' in text:
                    whois_data['page_description'] = value

        time.sleep(WHOIS_REQUEST_DELAY)

    except Exception as e:
        whois_data['whois_error'] = f'Error: {str(e)}'

    return whois_data


def process_whois_for_domains(all_data: List[Dict]) -> List[Dict]:
    """Обрабатывает WHOIS данные для уникальных доменов"""

    # Извлекаем уникальные домены
    unique_domains = set()

    for item in all_data:
        # Пробуем извлечь домен из green_url
        domain = extract_domain_from_url(item.get('green_url'))
        if domain:
            unique_domains.add(domain)
        else:
            # Если не получилось, пробуем из url
            domain = extract_domain_from_url(item.get('url'))
            if domain:
                unique_domains.add(domain)

    unique_domains = list(unique_domains)
    print(f"\n[WHOIS] Найдено уникальных доменов для проверки: {len(unique_domains)}")

    if not unique_domains:
        print("[WHOIS] Нет доменов для проверки")
        return all_data

    # Получаем WHOIS данные для каждого домена
    whois_results = {}

    for i, domain in enumerate(unique_domains, 1):
        print(f"[WHOIS] Обрабатываю домен {i}/{len(unique_domains)}: {domain}")
        whois_data = get_whois_data(domain)
        whois_results[domain] = whois_data

        if whois_data['whois_error']:
            print(f"[WHOIS] Ошибка для {domain}: {whois_data['whois_error']}")
        else:
            print(f"[WHOIS] Успешно получены данные для {domain}")

    # Добавляем WHOIS данные к основным результатам
    enhanced_data = []

    for item in all_data:
        enhanced_item = item.copy()

        # Определяем домен для данного элемента
        domain = extract_domain_from_url(item.get('green_url'))
        if not domain:
            domain = extract_domain_from_url(item.get('url'))

        if domain and domain in whois_results:
            whois_info = whois_results[domain]
            # Добавляем все WHOIS поля к элементу
            enhanced_item.update({
                'whois_domain': whois_info['domain'],
                'whois_citation_index': whois_info['citation_index'],
                'whois_alexa_rating': whois_info['alexa_rating'],
                'whois_registrar': whois_info['registrar'],
                'whois_registration_date': whois_info['registration_date'],
                'whois_expiration_date': whois_info['expiration_date'],
                'whois_days_until_expiration': whois_info['days_until_expiration'],
                'whois_check_date': whois_info['check_date'],
                'whois_external_links': whois_info['external_links'],
                'whois_internal_links': whois_info['internal_links'],
                'whois_total_anchors': whois_info['total_anchors'],
                'whois_outgoing_anchors': whois_info['outgoing_anchors'],
                'whois_domain_links': whois_info['domain_links'],
                'whois_page_title': whois_info['page_title'],
                'whois_page_description': whois_info['page_description'],
                'whois_error': whois_info['whois_error']
            })
        else:
            # Добавляем пустые WHOIS поля
            enhanced_item.update({
                'whois_domain': None,
                'whois_citation_index': None,
                'whois_alexa_rating': None,
                'whois_registrar': None,
                'whois_registration_date': None,
                'whois_expiration_date': None,
                'whois_days_until_expiration': None,
                'whois_check_date': None,
                'whois_external_links': None,
                'whois_internal_links': None,
                'whois_total_anchors': None,
                'whois_outgoing_anchors': None,
                'whois_domain_links': None,
                'whois_page_title': None,
                'whois_page_description': None,
                'whois_error': 'No domain found'
            })

        enhanced_data.append(enhanced_item)

    return enhanced_data


def banner_wait_for_user(reason: str):
    print("\n" + "=" * 60)
    print("ACTION REQUIRED:", reason)
    input("Когда результаты видны и капча пройдена, нажми Enter...")
    print("=" * 60 + "\n")


def ensure_results_visible(page):
    while True:
        try:
            page.wait_for_load_state('domcontentloaded', timeout=10000)
        except Exception:
            pass
        try:
            if page.query_selector(RESULT_ITEM_SELECTOR):
                return True
            page.wait_for_selector(RESULT_ITEM_SELECTOR, timeout=WAIT_SELECTOR_TIMEOUT)
            return True
        except Exception:
            banner_wait_for_user("Похоже, капча или результаты не видны. Нужна ручная проверка.")


def reveal_phones(page):
    try:
        buttons = page.query_selector_all(PHONE_BUTTON_SELECTOR)
    except Exception:
        buttons = []
    if not buttons:
        return
    print(f"[phones] Найдено кнопок для раскрытия телефонов: {len(buttons)}")
    for btn in buttons:
        try:
            btn.scroll_into_view_if_needed()
            try:
                btn.click(timeout=3000)
            except Exception:
                page.evaluate("(el) => el.click()", btn)
            time.sleep(0.4)
        except Exception as e:
            print(f"[phones] Ошибка при клике: {e}")


def extract_rating_and_reviews(result):
    """Извлекает рейтинг и отзывы из различных форматов"""
    rating = None
    reviews_count = None

    # Поиск контейнера с рейтингом - новый подход
    rating_containers = [
        result.select_one('div.OrganicUgcReviews'),
        result.select_one('div.ShopInfo-Ugc'),
        result.select_one('div.VanillaReact.OrganicUgcReviews'),
        result.select_one('div[data-vnl*="rate"]')
    ]

    for container in rating_containers:
        if not container:
            continue

        # Попытка извлечь из data-vnl атрибута
        data_vnl = container.get('data-vnl')
        if data_vnl:
            try:
                # Очистка HTML entities
                cleaned_json = data_vnl.replace('&quot;', '"').replace('&amp;', '&').replace('&nbsp;', ' ')
                data = json.loads(cleaned_json)

                # Поиск рейтинга в JSON
                if 'ecomRates' in data:
                    ecom_data = data['ecomRates']
                    if 'rate' in ecom_data:
                        rating = str(ecom_data['rate']).replace(',', '.')
                    if 'reviewCount' in ecom_data:
                        reviews_count = str(ecom_data['reviewCount'])
                    elif 'reviewCountText' in ecom_data:
                        reviews_text = ecom_data['reviewCountText']
                        # Извлекаем число из текста типа "13 473 отзыва"
                        numbers = re.findall(r'[\d\s]+', reviews_text.replace('\u00a0', ' '))
                        if numbers:
                            reviews_count = ''.join(numbers).replace(' ', '').strip()

                if rating and reviews_count:
                    return rating, reviews_count

            except Exception as e:
                print(f"[rating] Ошибка парсинга JSON из data-vnl: {e}")

        # Если JSON не сработал, пробуем извлечь из HTML
        # Поиск рейтинга
        rating_elements = [
            container.select_one('.RatingOneStar .Line-AddonContent'),
            container.select_one('.RatingOneStar div[aria-hidden="true"]'),
            container.select_one('.OrganicUgcReviews-Rating .Line-AddonContent'),
            container.select_one('span.A11yHidden')
        ]

        for elem in rating_elements:
            if elem and not rating:
                text = elem.get_text(strip=True)
                if 'рейтинг' in text.lower():
                    # Извлекаем число из текста типа "Рейтинг: 4,7 из 5"
                    match = re.search(r'[\d,\.]+', text)
                    if match:
                        rating = match.group().replace(',', '.')
                elif re.match(r'^[\d,\.]+$', text):
                    rating = text.replace(',', '.')

        # Поиск отзывов
        reviews_elements = [
            container.select_one('.EReviews'),
            container.select_one('.OrganicUgcReviews-Text'),
            container.select_one('div[class*="Reviews"]')
        ]

        for elem in reviews_elements:
            if elem and not reviews_count:
                text = elem.get_text(strip=True)
                # Извлекаем числа из текста типа "13,5K отзывов", "1234 отзыва"
                if 'отзыв' in text.lower():
                    # Обработка форматов типа "13,5K", "1.2K", "123"
                    numbers = re.findall(r'[\d,\.]+[KkКк]?', text)
                    if numbers:
                        num_text = numbers[0]
                        if 'k' in num_text.lower() or 'к' in num_text.lower():
                            # Конвертируем K в число
                            base_num = float(
                                num_text.replace('K', '').replace('k', '').replace('К', '').replace('к', '').replace(
                                    ',', '.'))
                            reviews_count = str(int(base_num * 1000))
                        else:
                            reviews_count = num_text.replace(',', '').replace('.', '')

        if rating and reviews_count:
            break

    return rating, reviews_count


def parse_page(html_content: str, page_number: int, parsed_date: str):
    soup = BeautifulSoup(html_content, 'html.parser')
    results = soup.select(RESULT_ITEM_SELECTOR)
    if not results:
        for alt in ALT_RESULT_SELECTORS:
            results = soup.select(alt)
            if results:
                break
    page_data = []
    for idx, result in enumerate(results, start=1):
        data = {
            'title': None,
            'url': None,
            'green_url': None,
            'description': None,
            'phone': None,
            'work_time': None,
            'rating': None,
            'reviews_count': None,
            'page_number': page_number,
            'parsed_date': parsed_date
        }
        try:
            # Title + URL
            title_element = (
                    result.select_one('h2.OrganicTitle-LinkText') or
                    result.select_one('h2.organic__title-wrapper span.organic__title') or
                    result.select_one('h2 a') or
                    result.select_one('a.OrganicTitle-Link') or
                    result.select_one('a.organic__url')
            )
            if title_element:
                if title_element.name == 'a':
                    data['title'] = title_element.get_text(strip=True)
                    data['url'] = title_element.get('href')
                else:
                    data['title'] = title_element.get_text(strip=True)
                    link = result.select_one('h2 a') or result.select_one('a.OrganicTitle-Link') or result.select_one(
                        'a.organic__url')
                    if link:
                        data['url'] = link.get('href')

            # Green URL
            green_url_element = (
                    result.select_one('a.organic__greenurl') or
                    result.select_one('div.Path') or
                    result.select_one('div.Organic-Path') or
                    result.select_one('span.organic__url-text')
            )
            if green_url_element:
                data['green_url'] = green_url_element.get_text(strip=True)

            # Description
            description_element = (
                    result.select_one('div.OrganicText') or
                    result.select_one('span.OrganicTextContentSpan') or
                    result.select_one('div.TextContainer span') or
                    result.select_one('div.organic__snippet') or
                    result.select_one('div.organic__content')
            )
            if description_element:
                data['description'] = description_element.get_text(strip=True)

            # Phone
            phone_full = result.select_one(PHONE_FULL_SELECTOR)
            if phone_full and phone_full.get_text(strip=True):
                data['phone'] = phone_full.get_text(strip=True)
            else:
                phone_element = result.select_one(PHONE_SPAN_SELECTOR)
                if phone_element:
                    text_inside = phone_element.get_text(strip=True)
                    if text_inside and 'показ' not in text_inside.lower():
                        data['phone'] = text_inside
                    else:
                        phone_json_str = phone_element.get("data-vnl") or phone_element.get("data-ibkqd")
                        if phone_json_str:
                            try:
                                cleaned = phone_json_str.replace('&quot;', '"').replace('&amp;', '&')
                                phone_json = json.loads(cleaned)
                                if isinstance(phone_json, dict):
                                    data['phone'] = phone_json.get('fullText')
                            except Exception:
                                pass

            # Work time (полный текст блока)
            meta_items = result.select('div.Meta-Item')
            for mi in meta_items:
                h3 = mi.select_one('h3.A11yHidden')
                if h3 and 'время работы' in h3.get_text(strip=True).lower():
                    data['work_time'] = mi.get_text(" ", strip=True)
                    break

            # Rating + reviews - ИСПРАВЛЕННАЯ ВЕРСИЯ
            rating, reviews_count = extract_rating_and_reviews(result)
            data['rating'] = rating
            data['reviews_count'] = reviews_count

            if data['title'] and data['url']:
                page_data.append(data)

            # Отладочный вывод для первых нескольких элементов
            if page_number == 1 and idx <= 3:
                print(
                    f"[debug] Элемент #{idx}: title='{data['title'][:50] if data['title'] else None}...', rating='{data['rating']}', reviews='{data['reviews_count']}'")

        except Exception as e:
            print(f"[page {page_number}] Ошибка парсинга элемента #{idx}: {e}")
            traceback.print_exc(file=sys.stdout)
            continue
    return page_data


def find_next_button(page):
    for sel in NEXT_BTN_SELECTORS:
        try:
            btn = page.query_selector(sel)
            if btn:
                return btn
        except Exception:
            continue
    return None


def ask_yes_no(question: str) -> bool:
    """Задает пользователю вопрос и возвращает True/False"""
    while True:
        answer = input(f"{question} (y/n): ").lower().strip()
        if answer in ['y', 'yes', 'да', 'д']:
            return True
        elif answer in ['n', 'no', 'нет', 'н']:
            return False
        else:
            print("Пожалуйста, введите 'y' для да или 'n' для нет")


def main():
    pages_to_scrape = max(1, int(PAGES_TO_SCRAPE))
    print(f"PAGES_TO_SCRAPE = {pages_to_scrape}")
    all_scraped_data = []
    parsed_date = datetime.now().strftime("%Y-%m-%d")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False, slow_mo=SLOW_MO_MS)
        page = browser.new_page()
        encoded_query = quote_plus(SEARCH_QUERY)
        start_url = f"https://yandex.ru/search/?text={encoded_query}"
        print(f"Открываю результаты поиска по запросу: '{SEARCH_QUERY}'")
        page.goto(start_url)
        time.sleep(PAUSE_AFTER_NAV_SEC)
        banner_wait_for_user("Старт работы. Если есть капча — пройди её, дождись результатов.")
        ensure_results_visible(page)

        for i in range(pages_to_scrape):
            page_num = i + 1
            print(f"\n--- Парсинг страницы {page_num} ---")
            ensure_results_visible(page)
            reveal_phones(page)
            html_content = page.content()
            page_data = parse_page(html_content, page_num, parsed_date)
            print(f"[page {page_num}] Найдено {len(page_data)} результатов.")

            # Подсчет результатов с рейтингами
            with_ratings = sum(1 for item in page_data if item['rating'])
            print(f"[page {page_num}] Результатов с рейтингами: {with_ratings}")

            all_scraped_data.extend(page_data)
            if AUTOSAVE_AFTER_EACH_PAGE:
                df = pd.DataFrame(all_scraped_data)
                df.to_csv(CSV_FILE_NAME, index=False, encoding='utf-8-sig')

            if page_num < pages_to_scrape:
                next_btn = find_next_button(page)
                if next_btn:
                    try:
                        next_btn.scroll_into_view_if_needed()
                    except Exception:
                        pass
                    try:
                        next_btn.click(timeout=5000)
                    except Exception:
                        page.evaluate("(el) => el.click()", next_btn)
                    page.wait_for_load_state('domcontentloaded')
                    time.sleep(PAUSE_AFTER_NAV_SEC)
                else:
                    print("[nav] Кнопка следующей страницы не найдена. Останавливаюсь.")
                    break

        print("Закрываю браузер...")
        browser.close()

    print("\nГотово. Сводим результаты...")
    if all_scraped_data:
        df = pd.DataFrame(all_scraped_data)
        df.to_csv(CSV_FILE_NAME, index=False, encoding='utf-8-sig')

        # Статистика по рейтингам
        total_results = len(all_scraped_data)
        with_ratings = len([item for item in all_scraped_data if item['rating']])
        print(f"Данные сохранены в {CSV_FILE_NAME} (строк: {total_results})")
        print(f"Результатов с рейтингами: {with_ratings} из {total_results}")

        # Спрашиваем про WHOIS проверку
        print("\n" + "=" * 60)
        print("ОСНОВНОЙ ПАРСИНГ ЗАВЕРШЕН")
        print("=" * 60)

        if ask_yes_no("Хотите проверить домены через WHOIS.ru?"):
            print("\n[WHOIS] Начинаю проверку доменов...")
            enhanced_data = process_whois_for_domains(all_scraped_data)

            # Сохраняем расширенные данные
            df_enhanced = pd.DataFrame(enhanced_data)
            df_enhanced.to_csv(CSV_WITH_WHOIS_NAME, index=False, encoding='utf-8-sig')

            print(f"\n[WHOIS] Данные с WHOIS информацией сохранены в {CSV_WITH_WHOIS_NAME}")

            # Статистика по WHOIS
            successful_whois = len(
                [item for item in enhanced_data if item.get('whois_domain') and not item.get('whois_error')])
            failed_whois = len([item for item in enhanced_data if
                                item.get('whois_error') and item.get('whois_error') != 'No domain found'])
            no_domain = len([item for item in enhanced_data if item.get('whois_error') == 'No domain found'])

            print(f"[WHOIS] Успешно обработано доменов: {successful_whois}")
            print(f"[WHOIS] Ошибок при обработке: {failed_whois}")
            print(f"[WHOIS] Результатов без доменов: {no_domain}")
        else:
            print("[WHOIS] Проверка доменов пропущена.")

    else:
        print("Результатов нет — файл не создан.")


if __name__ == "__main__":
    main()
