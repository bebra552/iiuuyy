import asyncio
import aiohttp
from aiohttp import ClientSession, TCPConnector
from urllib.parse import urlparse
import csv
import os
import re
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import random
import ssl
import logging
from typing import Optional

# --- Настройка логирования для более чистого вывода ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Основные константы ---
BASE = "https://bigspr.ru"
INPUT_FILE = r"C:\\Users\\User\\Desktop\\ssilki.txt"
OUTPUT_FILE = "companies_full_fast.csv"

# --- Настройки для максимальной скорости ---
HTTP_CONCURRENCY = 50
CATEGORY_CONCURRENCY = 8
COMPANY_CONCURRENCY = 10
REQUEST_TIMEOUT = 30
MAX_RETRIES = 3
RETRY_DELAY = 2

# DNS cache TTL for connector (seconds)
DNS_CACHE_TTL = 300

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]


def get_random_headers() -> dict:
    ua = random.choice(USER_AGENTS)
    return {
        "User-Agent": ua,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept-Encoding": "gzip, deflate, br, zstd",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Cache-Control": "max-age=0",
        "DNT": "1",
    }


http_sem = asyncio.Semaphore(HTTP_CONCURRENCY)

# Reuse SSL context and client timeout across requests
SSL_CONTEXT = ssl.create_default_context()
CLIENT_TIMEOUT = aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)

# Precompiled regex for company URL detection
COMPANY_ID_RE = re.compile(r"-\d{6,}")

# Base headers (including a single randomized User-Agent) to be set per session
BASE_HEADERS: Optional[dict] = None

# Try to use uvloop if available (faster and lower CPU overhead)
try:
    import uvloop  # type: ignore
    asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
except Exception:
    pass


def ensure_trailing_slash(url: str) -> str:
    return url if url.endswith("/") else url + "/"


def is_company_url(path: str) -> bool:
    parts = [p for p in path.split("/") if p]
    if len(parts) < 2:
        return False
    return any(COMPANY_ID_RE.search(part) for part in parts)


def extract_city_from_rubric(rubric_url: str) -> str | None:
    try:
        path = urlparse(rubric_url).path
        parts = [p for p in path.split("/") if p]
        return parts[0] if parts else None
    except Exception:
        return None


async def fetch_text(session: ClientSession, url: str) -> str | None:
    for attempt in range(MAX_RETRIES):
        try:
            parsed = urlparse(url)
            # Use session-level headers and only add Referer per request
            headers = (BASE_HEADERS or get_random_headers()).copy()
            headers["Referer"] = f"{parsed.scheme}://{parsed.netloc}/"

            async with http_sem:
                async with session.get(url, headers=headers) as resp:
                    # Skip expensive charset detection; site is UTF-8
                    text = await resp.text(encoding='utf-8', errors='ignore')
                    if "document.cookie='beget=begetok'" in text:
                        logging.info(f"Обнаружена защита Beget. Устанавливаю cookie для {url}")
                        session.cookie_jar.update_cookies({'beget': 'begetok'})
                        await asyncio.sleep(0.5)
                        continue
                    if resp.status == 200:
                        return text
                    logging.warning(f"Статус {resp.status} при запросе к {url} (попытка {attempt + 1})")
        except Exception as e:
            logging.error(f"Ошибка при запросе к {url} (попытка {attempt + 1}): {e}")
        if attempt < MAX_RETRIES - 1:
            await asyncio.sleep(RETRY_DELAY)
    logging.error(f"Не удалось получить данные с {url} после {MAX_RETRIES} попыток.")
    return None


async def parse_sitemap_xml(session: ClientSession, url: str) -> list[str]:
    logging.info(f"Загружаю sitemap: {url}")
    text = await fetch_text(session, url)
    if not text: return []
    try:
        root = ET.fromstring(text.encode('utf-8'))
        locs = [el.text.strip() for el in root.findall(".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc") if
                el.text]
        logging.info(f"Найдено {len(locs)} ссылок в sitemap: {url}")
        return locs
    except ET.ParseError:
        logging.error(f"Ошибка парсинга XML в {url}.")
        return []


async def discover_city_rubrics_from_input(session: ClientSession, input_url: str) -> list[str]:
    url = input_url.strip()
    if url.endswith(".xml") and "/sitemap/" in url:
        return await parse_sitemap_xml(session, url)
    logging.warning(f"Неопознанный тип ссылки: {url}")
    return []


async def parse_company(session: ClientSession, company_url: str, city: str | None, rubric_url: str) -> dict | None:
    html_text = await fetch_text(session, company_url)
    if not html_text:
        return None

    try:
        soup = BeautifulSoup(html_text, "lxml")
        data = {
            "city": city,
            "rubric_url": rubric_url,
            "url": company_url,
            "name": None,
            "address": None,
            "phones": None,
            "schedule": None,
            "website": None,
            "socials": None,
            "email": None,
            "categories": None
        }

        # Получаем название компании
        h1 = soup.select_one("h1")
        if h1:
            data["name"] = h1.get_text(strip=True)
        else:
            data["name"] = "Название не найдено"

        # Ищем контакты - используем более широкий поиск
        contacts_panel = soup.select_one(".panel-body .item-text")
        if not contacts_panel:
            # Альтернативный поиск контактов
            contacts_panel = soup.select_one(".item-text")
            if not contacts_panel:
                contacts_panel = soup.select_one(".panel-body")

        # Собираем параграфы либо из панели контактов, либо из всего документа (fallback)
        candidate_paragraphs = contacts_panel.find_all("p") if contacts_panel else soup.find_all("p")
        if candidate_paragraphs:
            for p_tag in candidate_paragraphs:
                text = p_tag.get_text(" ", strip=True)
                # Быстрый поиск по классам иконок без преобразования в HTML-строку
                icon_span = p_tag.find("span", class_=True)
                icon_classes = icon_span.get("class", []) if icon_span else []

                matched = False
                if "glyphicon-map-marker" in icon_classes:
                    data["address"] = text; matched = True
                elif "glyphicon-earphone" in icon_classes:
                    data["phones"] = text; matched = True
                elif "glyphicon-time" in icon_classes:
                    data["schedule"] = text; matched = True
                elif "glyphicon-share" in icon_classes:
                    data["website"] = text; matched = True
                elif "glyphicon-link" in icon_classes:
                    links = [a.get("href") for a in p_tag.find_all("a", href=True)]
                    data["socials"] = ", ".join(links) if links else text; matched = True
                elif "glyphicon-envelope" in icon_classes:
                    data["email"] = text; matched = True
                elif "glyphicon-list" in icon_classes:
                    cats = [a.get_text(strip=True) for a in p_tag.find_all("a")]
                    data["categories"] = ", ".join(cats) if cats else text; matched = True

                # Редкий случай: запасной путь через поиск по HTML (миграционная совместимость)
                if not matched:
                    p_html = str(p_tag)
                    if "glyphicon-map-marker" in p_html:
                        data["address"] = text
                    elif "glyphicon-earphone" in p_html:
                        data["phones"] = text
                    elif "glyphicon-time" in p_html:
                        data["schedule"] = text
                    elif "glyphicon-share" in p_html:
                        data["website"] = text
                    elif "glyphicon-link" in p_html:
                        links = [a.get("href") for a in p_tag.find_all("a", href=True)]
                        data["socials"] = ", ".join(links) if links else text
                    elif "glyphicon-envelope" in p_html:
                        data["email"] = text
                    elif "glyphicon-list" in p_html:
                        cats = [a.get_text(strip=True) for a in p_tag.find_all("a")]
                        data["categories"] = ", ".join(cats) if cats else text

        logging.debug(
            "    ↳ Собраны данные для компании: %s | Адрес: %s | Телефон: %s",
            data['name'], bool(data['address']), bool(data['phones'])
        )
        return data
    except Exception as e:
        logging.error(f"Ошибка при парсинге карточки компании {company_url}: {e}")
        return None


async def parse_category(session: ClientSession, rubric_url: str, csv_writer, lock: asyncio.Lock):
    rubric_url = ensure_trailing_slash(rubric_url)
    city = extract_city_from_rubric(rubric_url)
    rubric_name = rubric_url.rstrip('/').split('/')[-1]

    logging.info(f"Начинаю обработку рубрики: {city}/{rubric_name}")
    page = 1
    total_companies_in_rubric = 0

    while True:
        page_url = f"{rubric_url}page-{page}/" if page > 1 else rubric_url
        html_text = await fetch_text(session, page_url)
        if not html_text:
            break

        soup = BeautifulSoup(html_text, "lxml")
        company_links = [a['href'] for a in soup.select(".panel.panel-info.object .panel-heading a[href]")]

        if not company_links:
            if page > 1:
                logging.info(f"Достигнут конец рубрики {city}/{rubric_name} на странице {page}.")
            break

        # Собираем абсолютные ссылки и удаляем дубликаты, сохраняя порядок
        seen_urls = set()
        company_urls = []
        for href in company_links:
            abs_url = BASE + href if href.startswith('/') else href
            if abs_url not in seen_urls:
                seen_urls.add(abs_url)
                company_urls.append(abs_url)
        logging.info(
            f"  Страница {page} [{city}/{rubric_name}]: найдено {len(company_urls)} компаний. Начинаю сбор данных...")

        # Ограничиваем параллелизм для парсинга компаний
        sem = asyncio.Semaphore(COMPANY_CONCURRENCY)

        async def _parse_with_sem(url):
            async with sem:
                return await parse_company(session, url, city, rubric_url)

        tasks = [_parse_with_sem(url) for url in company_urls]
        results = await asyncio.gather(*tasks)
        valid_results = [res for res in results if res]

        if valid_results:
            async with lock:
                csv_writer.writerows(valid_results)
            logging.info(
                f"  Страница {page} [{city}/{rubric_name}]: СОХРАНЕНО {len(valid_results)}/{len(company_urls)} компаний в CSV.")
            total_companies_in_rubric += len(valid_results)

        # Небольшая пауза между страницами для снижения нагрузки, но быстрее, чем раньше
        await asyncio.sleep(random.uniform(0.02, 0.05))
        page += 1

    logging.info(f"✅ Рубрика {city}/{rubric_name} завершена. Всего собрано: {total_companies_in_rubric} компаний.")


async def main():
    start_time = asyncio.get_event_loop().time()
    logging.info("🚀 Запуск парсера BigSpr...")

    if not os.path.exists(INPUT_FILE):
        logging.critical(f"❌ Файл не найден: {INPUT_FILE}")
        return

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        input_lines = [line.strip() for line in f if line.strip()]

    connector = TCPConnector(
        limit=HTTP_CONCURRENCY,
        limit_per_host=HTTP_CONCURRENCY,
        ttl_dns_cache=DNS_CACHE_TTL,
        ssl=SSL_CONTEXT,
        enable_cleanup_closed=True,
    )

    # Инициализируем единые заголовки для всей сессии (случайный User-Agent один раз)
    global BASE_HEADERS
    BASE_HEADERS = get_random_headers()

    async with ClientSession(connector=connector, timeout=CLIENT_TIMEOUT, headers=BASE_HEADERS) as session:
        logging.info("--- ЭТАП 1: Сбор URL рубрик из sitemap-ов ---")
        discovery_tasks = [discover_city_rubrics_from_input(session, line) for line in input_lines]
        results = await asyncio.gather(*discovery_tasks)

        # Дедупликация рубрик без сортировки (сохраняем порядок обнаружения)
        rubric_urls = []
        seen_rubrics = set()
        for url_list in results:
            for url in url_list:
                if is_company_url(urlparse(url).path):
                    continue
                if url not in seen_rubrics:
                    seen_rubrics.add(url)
                    rubric_urls.append(url)

        if not rubric_urls:
            logging.critical("❌ Не найдено ни одной рубрики для парсинга.")
            return

        logging.info(f"✅ ИТОГ ЭТАПА 1: Найдено {len(rubric_urls)} уникальных рубрик для парсинга.")

        fieldnames = ["city", "rubric_url", "url", "name", "address", "phones", "schedule", "website", "socials",
                      "email", "categories"]
        csv_lock = asyncio.Lock()

        logging.info(f"\n--- ЭТАП 2: Парсинг компаний из рубрик ---")
        try:
            # Увеличиваем буферизацию записи в файл для уменьшения нагрузки на диск
            with open(OUTPUT_FILE, "w", newline="", encoding="utf-8", buffering=1024*1024) as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                logging.info(f"Создан CSV файл: {OUTPUT_FILE}")

                sem_category = asyncio.Semaphore(CATEGORY_CONCURRENCY)
                progress_counter = 0
                progress_lock = asyncio.Lock()

                async def run_category(rurl):
                    nonlocal progress_counter
                    async with sem_category:
                        await parse_category(session, rurl, writer, csv_lock)
                    async with progress_lock:
                        progress_counter += 1
                        progress = (progress_counter / len(rubric_urls)) * 100
                        logging.info(
                            f"📈 ПРОГРЕСС: {progress_counter}/{len(rubric_urls)} ({progress:.1f}%) | Время: {asyncio.get_event_loop().time() - start_time:.0f}с")

                await asyncio.gather(*[run_category(r) for r in rubric_urls])
        except IOError as e:
            logging.critical(f"Ошибка записи в файл {OUTPUT_FILE}: {e}")

    total_time = asyncio.get_event_loop().time() - start_time
    try:
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            total_companies = sum(1 for _ in f) - 1
    except FileNotFoundError:
        total_companies = 0

    logging.info("\n" + "=" * 50 + "\n🎉 ПАРСИНГ ЗАВЕРШЕН!\n" + "=" * 50)
    logging.info(f"📊 Обработано рубрик: {len(rubric_urls)}")
    logging.info(f"🏢 Собрано компаний: {total_companies}")
    logging.info(f"⏱️ Общее время: {total_time:.1f} секунд")
    if total_time > 0:
        logging.info(f"⚡ Средняя скорость: {total_companies / total_time:.1f} компаний/сек")
    logging.info(f"💾 Результат сохранен в: {OUTPUT_FILE}")


if __name__ == "__main__":
    asyncio.run(main())
