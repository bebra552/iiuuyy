import asyncio
import aiohttp
from aiohttp import ClientSession, TCPConnector
from urllib.parse import urlparse
import csv
import os
import re
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import random
import ssl
import logging

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –±–æ–ª–µ–µ —á–∏—Å—Ç–æ–≥–æ –≤—ã–≤–æ–¥–∞ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
BASE = "https://bigspr.ru"
INPUT_FILE = r"C:\\Users\\User\\Desktop\\ssilki.txt"
OUTPUT_FILE = "companies_full_fast.csv"

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ ---
HTTP_CONCURRENCY = 50
CATEGORY_CONCURRENCY = 8
COMPANY_CONCURRENCY = 10
REQUEST_TIMEOUT = 30
MAX_RETRIES = 3
RETRY_DELAY = 2

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]


def get_random_headers() -> dict:
    ua = random.choice(USER_AGENTS)
    return {
        "User-Agent": ua,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept-Encoding": "gzip, deflate, br, zstd",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Cache-Control": "max-age=0",
        "DNT": "1",
    }


http_sem = asyncio.Semaphore(HTTP_CONCURRENCY)


def ensure_trailing_slash(url: str) -> str:
    return url if url.endswith("/") else url + "/"


def is_company_url(path: str) -> bool:
    parts = [p for p in path.split("/") if p]
    if len(parts) < 2: return False
    return any(re.search(r"-\d{6,}", part) for part in parts)


def extract_city_from_rubric(rubric_url: str) -> str | None:
    try:
        path = urlparse(rubric_url).path
        parts = [p for p in path.split("/") if p]
        return parts[0] if parts else None
    except Exception:
        return None


async def fetch_text(session: ClientSession, url: str) -> str | None:
    for attempt in range(MAX_RETRIES):
        try:
            headers = get_random_headers()
            headers["Referer"] = f"{urlparse(url).scheme}://{urlparse(url).netloc}/"

            async with http_sem:
                async with session.get(
                        url, headers=headers, timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT),
                        ssl=ssl.create_default_context()
                ) as resp:
                    text = await resp.text()
                    if "document.cookie='beget=begetok'" in text:
                        logging.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞—â–∏—Ç–∞ Beget. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é cookie –¥–ª—è {url}")
                        session.cookie_jar.update_cookies({'beget': 'begetok'})
                        await asyncio.sleep(0.5)
                        continue
                    if resp.status == 200:
                        return text
                    logging.warning(f"–°—Ç–∞—Ç—É—Å {resp.status} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
        if attempt < MAX_RETRIES - 1:
            await asyncio.sleep(RETRY_DELAY)
    logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å {url} –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫.")
    return None


async def parse_sitemap_xml(session: ClientSession, url: str) -> list[str]:
    logging.info(f"–ó–∞–≥—Ä—É–∂–∞—é sitemap: {url}")
    text = await fetch_text(session, url)
    if not text: return []
    try:
        root = ET.fromstring(text.encode('utf-8'))
        locs = [el.text.strip() for el in root.findall(".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc") if
                el.text]
        logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(locs)} —Å—Å—ã–ª–æ–∫ –≤ sitemap: {url}")
        return locs
    except ET.ParseError:
        logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML –≤ {url}.")
        return []


async def discover_city_rubrics_from_input(session: ClientSession, input_url: str) -> list[str]:
    url = input_url.strip()
    if url.endswith(".xml") and "/sitemap/" in url:
        return sorted(await parse_sitemap_xml(session, url))
    logging.warning(f"–ù–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–∏–ø —Å—Å—ã–ª–∫–∏: {url}")
    return []


async def parse_company(session: ClientSession, company_url: str, city: str | None, rubric_url: str) -> dict | None:
    html_text = await fetch_text(session, company_url)
    if not html_text:
        return None

    try:
        soup = BeautifulSoup(html_text, "lxml")
        data = {
            "city": city,
            "rubric_url": rubric_url,
            "url": company_url,
            "name": None,
            "address": None,
            "phones": None,
            "schedule": None,
            "website": None,
            "socials": None,
            "email": None,
            "categories": None
        }

        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
        h1 = soup.select_one("h1")
        if h1:
            data["name"] = h1.get_text(strip=True)
        else:
            data["name"] = "–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

        # –ò—â–µ–º –∫–æ–Ω—Ç–∞–∫—Ç—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–π –ø–æ–∏—Å–∫
        contacts_panel = soup.select_one(".panel-body .item-text")
        if not contacts_panel:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
            contacts_panel = soup.select_one(".item-text")
            if not contacts_panel:
                contacts_panel = soup.select_one(".panel-body")

        if contacts_panel:
            for p_tag in contacts_panel.find_all("p"):
                text = p_tag.get_text(" ", strip=True)
                p_html = str(p_tag)  # –ü–æ–ª—É—á–∞–µ–º HTML –∫–æ–¥ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∏—Å–∫ –ø–æ HTML –∫–æ–¥—É (–∫–∞–∫ –≤–æ –≤—Ç–æ—Ä–æ–º —Ñ–∞–π–ª–µ)
                if "glyphicon-map-marker" in p_html:
                    data["address"] = text
                elif "glyphicon-earphone" in p_html:
                    data["phones"] = text
                elif "glyphicon-time" in p_html:
                    data["schedule"] = text
                elif "glyphicon-share" in p_html:
                    data["website"] = text
                elif "glyphicon-link" in p_html:
                    # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–æ—Ü—Å–µ—Ç–∏
                    links = [a.get("href") for a in p_tag.find_all("a", href=True)]
                    data["socials"] = ", ".join(links) if links else text
                elif "glyphicon-envelope" in p_html:
                    data["email"] = text
                elif "glyphicon-list" in p_html:
                    # –°–æ–±–∏—Ä–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                    cats = [a.get_text(strip=True) for a in p_tag.find_all("a")]
                    data["categories"] = ", ".join(cats) if cats else text

        logging.info(
            f"    ‚Ü≥ –°–æ–±—Ä–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–∏: {data['name']} | –ê–¥—Ä–µ—Å: {bool(data['address'])} | –¢–µ–ª–µ—Ñ–æ–Ω: {bool(data['phones'])}")
        return data
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –∫–æ–º–ø–∞–Ω–∏–∏ {company_url}: {e}")
        return None


async def parse_category(session: ClientSession, rubric_url: str, csv_writer, lock: asyncio.Lock):
    rubric_url = ensure_trailing_slash(rubric_url)
    city = extract_city_from_rubric(rubric_url)
    rubric_name = rubric_url.rstrip('/').split('/')[-1]

    logging.info(f"–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ä—É–±—Ä–∏–∫–∏: {city}/{rubric_name}")
    page = 1
    total_companies_in_rubric = 0

    while True:
        page_url = f"{rubric_url}page-{page}/" if page > 1 else rubric_url
        html_text = await fetch_text(session, page_url)
        if not html_text:
            break

        soup = BeautifulSoup(html_text, "lxml")
        company_links = [a['href'] for a in soup.select(".panel.panel-info.object .panel-heading a[href]")]

        if not company_links:
            if page > 1:
                logging.info(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü —Ä—É–±—Ä–∏–∫–∏ {city}/{rubric_name} –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}.")
            break

        company_urls = sorted(list(set(BASE + href if href.startswith('/') else href for href in company_links)))
        logging.info(
            f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} [{city}/{rubric_name}]: –Ω–∞–π–¥–µ–Ω–æ {len(company_urls)} –∫–æ–º–ø–∞–Ω–∏–π. –ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö...")

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–º–ø–∞–Ω–∏–π
        sem = asyncio.Semaphore(COMPANY_CONCURRENCY)

        async def _parse_with_sem(url):
            async with sem:
                return await parse_company(session, url, city, rubric_url)

        tasks = [_parse_with_sem(url) for url in company_urls]
        results = await asyncio.gather(*tasks)
        valid_results = [res for res in results if res]

        if valid_results:
            async with lock:
                csv_writer.writerows(valid_results)
            logging.info(
                f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} [{city}/{rubric_name}]: –°–û–•–†–ê–ù–ï–ù–û {len(valid_results)}/{len(company_urls)} –∫–æ–º–ø–∞–Ω–∏–π –≤ CSV.")
            total_companies_in_rubric += len(valid_results)

        await asyncio.sleep(random.uniform(0.1, 0.3))
        page += 1

    logging.info(f"‚úÖ –†—É–±—Ä–∏–∫–∞ {city}/{rubric_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {total_companies_in_rubric} –∫–æ–º–ø–∞–Ω–∏–π.")


async def main():
    start_time = asyncio.get_event_loop().time()
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ BigSpr...")

    if not os.path.exists(INPUT_FILE):
        logging.critical(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {INPUT_FILE}")
        return

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        input_lines = [line.strip() for line in f if line.strip()]

    connector = TCPConnector(limit=HTTP_CONCURRENCY, limit_per_host=HTTP_CONCURRENCY, enable_cleanup_closed=True)

    async with ClientSession(connector=connector) as session:
        logging.info("--- –≠–¢–ê–ü 1: –°–±–æ—Ä URL —Ä—É–±—Ä–∏–∫ –∏–∑ sitemap-–æ–≤ ---")
        discovery_tasks = [discover_city_rubrics_from_input(session, line) for line in input_lines]
        rubric_urls = sorted(list(set(url for url_list in await asyncio.gather(*discovery_tasks) for url in url_list if
                                      not is_company_url(urlparse(url).path))))

        if not rubric_urls:
            logging.critical("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π —Ä—É–±—Ä–∏–∫–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞.")
            return

        logging.info(f"‚úÖ –ò–¢–û–ì –≠–¢–ê–ü–ê 1: –ù–∞–π–¥–µ–Ω–æ {len(rubric_urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä—É–±—Ä–∏–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞.")

        fieldnames = ["city", "rubric_url", "url", "name", "address", "phones", "schedule", "website", "socials",
                      "email", "categories"]
        csv_lock = asyncio.Lock()

        logging.info(f"\n--- –≠–¢–ê–ü 2: –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ —Ä—É–±—Ä–∏–∫ ---")
        try:
            with open(OUTPUT_FILE, "w", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                logging.info(f"–°–æ–∑–¥–∞–Ω CSV —Ñ–∞–π–ª: {OUTPUT_FILE}")

                sem_category = asyncio.Semaphore(CATEGORY_CONCURRENCY)
                progress_counter = 0

                async def run_category(rurl):
                    nonlocal progress_counter
                    async with sem_category:
                        await parse_category(session, rurl, writer, csv_lock)
                    async with csv_lock:
                        progress_counter += 1
                        progress = (progress_counter / len(rubric_urls)) * 100
                        logging.info(
                            f"üìà –ü–†–û–ì–†–ï–°–°: {progress_counter}/{len(rubric_urls)} ({progress:.1f}%) | –í—Ä–µ–º—è: {asyncio.get_event_loop().time() - start_time:.0f}—Å")

                await asyncio.gather(*[run_category(r) for r in rubric_urls])
        except IOError as e:
            logging.critical(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª {OUTPUT_FILE}: {e}")

    total_time = asyncio.get_event_loop().time() - start_time
    try:
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            total_companies = sum(1 for _ in f) - 1
    except FileNotFoundError:
        total_companies = 0

    logging.info("\n" + "=" * 50 + "\nüéâ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!\n" + "=" * 50)
    logging.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä—É–±—Ä–∏–∫: {len(rubric_urls)}")
    logging.info(f"üè¢ –°–æ–±—Ä–∞–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {total_companies}")
    logging.info(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f} —Å–µ–∫—É–Ω–¥")
    if total_time > 0:
        logging.info(f"‚ö° –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {total_companies / total_time:.1f} –∫–æ–º–ø–∞–Ω–∏–π/—Å–µ–∫")
    logging.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {OUTPUT_FILE}")


if __name__ == "__main__":
    asyncio.run(main())